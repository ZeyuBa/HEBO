{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3936,"databundleVersionId":137277,"sourceType":"competition"}],"dockerImageVersionId":29985,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Predicting Forest Cover Type","metadata":{}},{"cell_type":"markdown","source":"### Thank you for opening this kernel!","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0"}},{"cell_type":"markdown","source":"> PROJECT CONTENT:\n1. Import Necessary Libraries\n2. Data Exploration/ Analysis/ Visualizing\n3. Correlation & Correlation Matrix\n4. Predictive Modeling\n5. Confusion Matrix\n6. Precision and Recall\n7. Hyperparameters Tuning\n8. Ensemble Methods","metadata":{}},{"cell_type":"markdown","source":"> Goal:\n* The goal of this competition is to predict Forest Cover Type. We will practice Classification Algorithms to achieve the lowest prediction error.","metadata":{}},{"cell_type":"markdown","source":"# Predictive Modeling:\n1. Logistic Regression\n2. KNN Classifier\n3. Gaussian Naive Bayes\n4. Support Vector Machine(SVM)\n5. Decision Tree\n6. Random Forest","metadata":{}},{"cell_type":"markdown","source":"### Import Necessary Libraries and Data Sets.","metadata":{}},{"cell_type":"code","source":"from subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Import the necessary packages\nimport numpy as np\nimport pandas as pd\n\nimport warnings\nwarnings.simplefilter(action =\"ignore\")\n\nfrom collections import Counter\n\n# Data visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scikitplot.plotters import plot_learning_curve\nfrom mlxtend.plotting import plot_learning_curves\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import ShuffleSplit\nfrom yellowbrick.model_selection import FeatureImportances\n\n# Algorithms\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn import metrics\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn import svm\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.metrics import mean_squared_error","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load Dataset\ntrain = pd.read_csv('../input/forest-cover-type-prediction/train.csv')\ntest  = pd.read_csv('../input/forest-cover-type-prediction/test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Exploration/Analysis","metadata":{}},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.columns.values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.columns.values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Analyse statically insight of train data\ntrain.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Analyse statically insight of test data\ntest.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"The train data size: {train.shape}\")\nprint(f\"The test data size: {test.shape}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Lets check that test dataset has all the columns in train dataset except Cover_Type","metadata":{}},{"cell_type":"code","source":"diff_train_test = set(train.columns) - set(test.columns)\ndiff_train_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The principal of this challenge is, Predict Cover_Type in the Roosevelt National Forest of northern Colorado. So get the info about the column of \"Cover_Type\":","metadata":{}},{"cell_type":"code","source":"train[\"Cover_Type\"].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(22,6), dpi= 80)\nax = sns.countplot(y=train[\"Cover_Type\"], hue=\"Cover_Type\", data=train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### As we can see in Top plot, each Cover_Type has same amount of records.","metadata":{}},{"cell_type":"markdown","source":"# Data Exploration/ Analysis/ Visualizing","metadata":{}},{"cell_type":"markdown","source":"> Correlation & Correlation Matrix\n##### Let's have a look first at the correlation between numerical features and the target \"Cover_Type\", in order to have a first idea of the connections between features. Just by looking at the heatmap below we can see many dark colors, many features have high correlation with the target.","metadata":{}},{"cell_type":"code","source":"numeric_data=train.select_dtypes(exclude=\"object\")\nnumeric_corr=numeric_data.corr()\nf,ax=plt.subplots(figsize=(19,1))\nsns.heatmap(numeric_corr.sort_values(by=[\"Cover_Type\"], ascending=False).head(1), cmap=\"Greens\")\nplt.title(\"Numerical features correlation with the Cover_Type\", weight=\"bold\", fontsize=18, color=\"darkgreen\")\nplt.yticks(weight=\"bold\", color=\"darkgreen\", rotation=0)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Num_feature=numeric_corr[\"Cover_Type\"].sort_values(ascending=False).head(20).to_frame()\n\ncm = sns.light_palette(\"forestgreen\", as_cmap=True)\n\nstyle = Num_feature.style.background_gradient(cmap=cm)\nstyle","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's do analysis of data and Check which features could contribute to a high Cover_Type rate:","metadata":{}},{"cell_type":"markdown","source":"1. Attribute Elevation vs Cover_Type\n> Plotting the distribution of the attribute Elevation for each tree","metadata":{}},{"cell_type":"code","source":"# Draw Plot\nplt.figure(figsize=(16,10), dpi= 80)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 1, \"Elevation\"], shade=True, color=\"#4169E1\", label=\"Cover=1\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 2, \"Elevation\"], shade=True, color=\"#FF8C00\", label=\"Cover=2\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 3, \"Elevation\"], shade=True, color=\"#FF4500\", label=\"Cover=3\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 4, \"Elevation\"], shade=True, color=\"#BDB76B\", label=\"Cover=4\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 5, \"Elevation\"], shade=True, color=\"#8B4513\", label=\"Cover=5\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 6, \"Elevation\"], shade=True, color=\"#9400D3\", label=\"Cover=6\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 7, \"Elevation\"], shade=True, color=\"#006400\", label=\"Cover=7\", alpha=.7)\n\n# Decoration\nplt.title(\"The distribution of the attribute Elevation\", color=\"#006400\", fontsize=22)\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> As we can see the Elevation feature has an important weight, Three covers with numbers 4,5 and 7 have already specified.\n* Use catplot() to combine a countplot() and a FacetGrid. This allows grouping \"Cover_Type\" and \"Elevation\".","metadata":{}},{"cell_type":"code","source":"g = sns.catplot(x=\"Elevation\", hue=\"Cover_Type\", col=\"Cover_Type\",\n                data=train, kind=\"count\",\n                height=4, aspect=.7);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Box plots are a great way to visualize the distribution, keeping the median, 25th 75th quartiles and the outliers in mind. ","metadata":{}},{"cell_type":"code","source":"# Draw Plot\nplt.figure(figsize=(15,8))\nsns.boxplot(x=\"Cover_Type\", y=\"Elevation\", data=train, hue=\"Cover_Type\")\n\n# Decoration\nplt.title(\"Box Plot of Elevation by Cover_Type\", fontsize=22, color=\"#006400\")\nplt.legend(title=\"Cover\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2. Attribute Aspect vs Cover_Type","metadata":{}},{"cell_type":"code","source":"# Draw Plot\nplt.figure(figsize=(16,10), dpi= 80)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 1, \"Aspect\"], shade=True, color=\"#4169E1\", label=\"Cover=1\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 2, \"Aspect\"], shade=True, color=\"#FF8C00\", label=\"Cover=2\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 3, \"Aspect\"], shade=True, color=\"#FF4500\", label=\"Cover=3\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 4, \"Aspect\"], shade=True, color=\"#BDB76B\", label=\"Cover=4\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 5, \"Aspect\"], shade=True, color=\"#8B4513\", label=\"Cover=5\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 6, \"Aspect\"], shade=True, color=\"#9400D3\", label=\"Cover=6\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 7, \"Aspect\"], shade=True, color=\"#006400\", label=\"Cover=7\", alpha=.7)\n\n# Decoration\nplt.title(\"The distribution of the attribute Aspect\", color=\"#006400\", fontsize=22)\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g = sns.catplot(x=\"Aspect\", hue=\"Cover_Type\", col=\"Cover_Type\",\n                data=train, kind=\"count\",\n                height=4, aspect=.7);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Draw Plot\nplt.figure(figsize=(15,8))\nsns.boxplot(x=\"Cover_Type\", y=\"Aspect\", data=train, hue=\"Cover_Type\")\n\n# Decoration\nplt.title(\"Box Plot of Aspect by Cover_Type\", fontsize=22, color=\"#006400\")\nplt.legend(title=\"Cover\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"3. Attribute Slope vs Cover_Type","metadata":{}},{"cell_type":"code","source":"# Draw Plot\nplt.figure(figsize=(16,10), dpi= 80)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 1, \"Slope\"], shade=True, color=\"#4169E1\", label=\"Cover=1\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 2, \"Slope\"], shade=True, color=\"#FF8C00\", label=\"Cover=2\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 3, \"Slope\"], shade=True, color=\"#FF4500\", label=\"Cover=3\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 4, \"Slope\"], shade=True, color=\"#BDB76B\", label=\"Cover=4\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 5, \"Slope\"], shade=True, color=\"#8B4513\", label=\"Cover=5\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 6, \"Slope\"], shade=True, color=\"#9400D3\", label=\"Cover=6\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 7, \"Slope\"], shade=True, color=\"#006400\", label=\"Cover=7\", alpha=.7)\n\n# Decoration\nplt.title(\"The distribution of the attribute Slope\", color=\"#006400\", fontsize=22)\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g = sns.catplot(x=\"Slope\", hue=\"Cover_Type\", col=\"Cover_Type\",\n                data=train, kind=\"count\",\n                height=4, aspect=.7);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Draw Plot\nplt.figure(figsize=(15,8))\nsns.boxplot(x=\"Cover_Type\", y=\"Slope\", data=train, hue=\"Cover_Type\")\n\n# Decoration\nplt.title(\"Box Plot of Slope by Cover_Type\", fontsize=22, color=\"#006400\")\nplt.legend(title=\"Cover\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"4. Attribute Horizontal_Distance_To_Hydrology vs Cover_Type","metadata":{}},{"cell_type":"code","source":"# Draw Plot\nplt.figure(figsize=(16,10), dpi= 80)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 1, \"Horizontal_Distance_To_Hydrology\"], shade=True, color=\"#4169E1\", label=\"Cover=1\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 2, \"Horizontal_Distance_To_Hydrology\"], shade=True, color=\"#FF8C00\", label=\"Cover=2\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 3, \"Horizontal_Distance_To_Hydrology\"], shade=True, color=\"#FF4500\", label=\"Cover=3\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 4, \"Horizontal_Distance_To_Hydrology\"], shade=True, color=\"#BDB76B\", label=\"Cover=4\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 5, \"Horizontal_Distance_To_Hydrology\"], shade=True, color=\"#8B4513\", label=\"Cover=5\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 6, \"Horizontal_Distance_To_Hydrology\"], shade=True, color=\"#9400D3\", label=\"Cover=6\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 7, \"Horizontal_Distance_To_Hydrology\"], shade=True, color=\"#006400\", label=\"Cover=7\", alpha=.7)\n\n# Decoration\nplt.title(\"The distribution of the attribute Horizontal_Distance_To_Hydrology\", color=\"#006400\", fontsize=22)\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g = sns.catplot(x=\"Horizontal_Distance_To_Hydrology\", hue=\"Cover_Type\", col=\"Cover_Type\",\n                data=train, kind=\"count\",\n                height=4, aspect=.7);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Draw Plot\nplt.figure(figsize=(15,8))\nsns.boxplot(x=\"Cover_Type\", y=\"Horizontal_Distance_To_Hydrology\", data=train, hue=\"Cover_Type\")\n\n# Decoration\nplt.title(\"Box Plot of Horizontal_Distance_To_Hydrology by Cover_Type\", fontsize=22, color=\"#006400\")\nplt.legend(title=\"Cover\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"5. Attribute Vertical_Distance_To_Hydrology vs Cover_Type","metadata":{}},{"cell_type":"code","source":"# Draw Plot\nplt.figure(figsize=(16,10), dpi= 80)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 1, \"Vertical_Distance_To_Hydrology\"], shade=True, color=\"#4169E1\", label=\"Cover=1\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 2, \"Vertical_Distance_To_Hydrology\"], shade=True, color=\"#FF8C00\", label=\"Cover=2\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 3, \"Vertical_Distance_To_Hydrology\"], shade=True, color=\"#FF4500\", label=\"Cover=3\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 4, \"Vertical_Distance_To_Hydrology\"], shade=True, color=\"#BDB76B\", label=\"Cover=4\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 5, \"Vertical_Distance_To_Hydrology\"], shade=True, color=\"#8B4513\", label=\"Cover=5\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 6, \"Vertical_Distance_To_Hydrology\"], shade=True, color=\"#9400D3\", label=\"Cover=6\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 7, \"Vertical_Distance_To_Hydrology\"], shade=True, color=\"#006400\", label=\"Cover=7\", alpha=.7)\n\n# Decoration\nplt.title(\"The distribution of the attribute Vertical_Distance_To_Hydrology\", color=\"#006400\", fontsize=22)\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Draw Plot\nplt.figure(figsize=(15,8))\nsns.boxplot(x=\"Cover_Type\", y=\"Vertical_Distance_To_Hydrology\", data=train, hue=\"Cover_Type\")\n\n# Decoration\nplt.title(\"Box Plot of Vertical_Distance_To_Hydrology by Cover_Type\", fontsize=22, color=\"#006400\")\nplt.legend(title=\"Cover\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"6. Attribute Horizontal_Distance_To_Roadways vs Cover_Type","metadata":{}},{"cell_type":"code","source":"# Draw Plot\nplt.figure(figsize=(16,10), dpi= 80)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 1, \"Horizontal_Distance_To_Roadways\"], shade=True, color=\"#4169E1\", label=\"Cover=1\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 2, \"Horizontal_Distance_To_Roadways\"], shade=True, color=\"#FF8C00\", label=\"Cover=2\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 3, \"Horizontal_Distance_To_Roadways\"], shade=True, color=\"#FF4500\", label=\"Cover=3\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 4, \"Horizontal_Distance_To_Roadways\"], shade=True, color=\"#BDB76B\", label=\"Cover=4\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 5, \"Horizontal_Distance_To_Roadways\"], shade=True, color=\"#8B4513\", label=\"Cover=5\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 6, \"Horizontal_Distance_To_Roadways\"], shade=True, color=\"#9400D3\", label=\"Cover=6\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 7, \"Horizontal_Distance_To_Roadways\"], shade=True, color=\"#006400\", label=\"Cover=7\", alpha=.7)\n\n# Decoration\nplt.title(\"The distribution of the attribute Horizontal_Distance_To_Roadways\", color=\"#006400\", fontsize=22)\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Draw Plot\nplt.figure(figsize=(15,8))\nsns.boxplot(x=\"Cover_Type\", y=\"Horizontal_Distance_To_Roadways\", data=train, hue=\"Cover_Type\")\n\n# Decoration\nplt.title(\"Box Plot of Horizontal_Distance_To_Roadways by Cover_Type\", fontsize=22, color=\"#006400\")\nplt.legend(title=\"Cover\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"7.  Attribute Hillshade_9am vs Cover_Type","metadata":{}},{"cell_type":"code","source":"# Draw Plot\nplt.figure(figsize=(16,10), dpi= 80)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 1, \"Hillshade_9am\"], shade=True, color=\"#4169E1\", label=\"Cover=1\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 2, \"Hillshade_9am\"], shade=True, color=\"#FF8C00\", label=\"Cover=2\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 3, \"Hillshade_9am\"], shade=True, color=\"#FF4500\", label=\"Cover=3\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 4, \"Hillshade_9am\"], shade=True, color=\"#BDB76B\", label=\"Cover=4\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 5, \"Hillshade_9am\"], shade=True, color=\"#8B4513\", label=\"Cover=5\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 6, \"Hillshade_9am\"], shade=True, color=\"#9400D3\", label=\"Cover=6\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 7, \"Hillshade_9am\"], shade=True, color=\"#006400\", label=\"Cover=7\", alpha=.7)\n\n# Decoration\nplt.title(\"The distribution of the attribute Hillshade_9am\", color=\"#006400\", fontsize=22)\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Draw Plot\nplt.figure(figsize=(15,8))\nsns.boxplot(x=\"Cover_Type\", y=\"Hillshade_9am\", data=train, hue=\"Cover_Type\")\n\n# Decoration\nplt.title(\"Box Plot of Hillshade_9am by Cover_Type\", fontsize=22, color=\"#006400\")\nplt.legend(title=\"Cover\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"8.  Attribute Hillshade_Noon vs Cover_Type","metadata":{}},{"cell_type":"code","source":"# Draw Plot\nplt.figure(figsize=(16,10), dpi= 80)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 1, \"Hillshade_Noon\"], shade=True, color=\"#4169E1\", label=\"Cover=1\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 2, \"Hillshade_Noon\"], shade=True, color=\"#FF8C00\", label=\"Cover=2\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 3, \"Hillshade_Noon\"], shade=True, color=\"#FF4500\", label=\"Cover=3\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 4, \"Hillshade_Noon\"], shade=True, color=\"#BDB76B\", label=\"Cover=4\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 5, \"Hillshade_Noon\"], shade=True, color=\"#8B4513\", label=\"Cover=5\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 6, \"Hillshade_Noon\"], shade=True, color=\"#9400D3\", label=\"Cover=6\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 7, \"Hillshade_Noon\"], shade=True, color=\"#006400\", label=\"Cover=7\", alpha=.7)\n\n# Decoration\nplt.title(\"The distribution of the attribute Hillshade_Noon\", color=\"#006400\", fontsize=22)\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Draw Plot\nplt.figure(figsize=(15,8))\nsns.boxplot(x=\"Cover_Type\", y=\"Hillshade_Noon\", data=train, hue=\"Cover_Type\")\n\n# Decoration\nplt.title(\"Box Plot of Hillshade_Noon by Cover_Type\", fontsize=22, color=\"#006400\")\nplt.legend(title=\"Cover\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"9.  Attribute Hillshade_3pm vs Cover_Type","metadata":{}},{"cell_type":"code","source":"# Draw Plot\nplt.figure(figsize=(16,10), dpi= 80)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 1, \"Hillshade_3pm\"], shade=True, color=\"#4169E1\", label=\"Cover=1\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 2, \"Hillshade_3pm\"], shade=True, color=\"#FF8C00\", label=\"Cover=2\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 3, \"Hillshade_3pm\"], shade=True, color=\"#FF4500\", label=\"Cover=3\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 4, \"Hillshade_3pm\"], shade=True, color=\"#BDB76B\", label=\"Cover=4\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 5, \"Hillshade_3pm\"], shade=True, color=\"#8B4513\", label=\"Cover=5\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 6, \"Hillshade_3pm\"], shade=True, color=\"#9400D3\", label=\"Cover=6\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 7, \"Hillshade_3pm\"], shade=True, color=\"#006400\", label=\"Cover=7\", alpha=.7)\n\n# Decoration\nplt.title(\"The distribution of the attribute Hillshade_3pm\", color=\"#006400\", fontsize=22)\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Draw Plot\nplt.figure(figsize=(15,8))\nsns.boxplot(x=\"Cover_Type\", y=\"Hillshade_3pm\", data=train, hue=\"Cover_Type\")\n\n# Decoration\nplt.title(\"Box Plot of Hillshade_3pm by Cover_Type\", fontsize=22, color=\"#006400\")\nplt.legend(title=\"Cover\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"10.  Attribute Horizontal_Distance_To_Fire_Points vs Cover_Type","metadata":{}},{"cell_type":"code","source":"# Draw Plot\nplt.figure(figsize=(16,10), dpi= 80)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 1, \"Horizontal_Distance_To_Fire_Points\"], shade=True, color=\"#4169E1\", label=\"Cover=1\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 2, \"Horizontal_Distance_To_Fire_Points\"], shade=True, color=\"#FF8C00\", label=\"Cover=2\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 3, \"Horizontal_Distance_To_Fire_Points\"], shade=True, color=\"#FF4500\", label=\"Cover=3\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 4, \"Horizontal_Distance_To_Fire_Points\"], shade=True, color=\"#BDB76B\", label=\"Cover=4\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 5, \"Horizontal_Distance_To_Fire_Points\"], shade=True, color=\"#8B4513\", label=\"Cover=5\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 6, \"Horizontal_Distance_To_Fire_Points\"], shade=True, color=\"#9400D3\", label=\"Cover=6\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 7, \"Horizontal_Distance_To_Fire_Points\"], shade=True, color=\"#006400\", label=\"Cover=7\", alpha=.7)\n\n# Decoration\nplt.title(\"The distribution of the attribute Horizontal_Distance_To_Fire_Points\", color=\"#006400\", fontsize=22)\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Draw Plot\nplt.figure(figsize=(15,8))\nsns.boxplot(x=\"Cover_Type\", y=\"Horizontal_Distance_To_Fire_Points\", data=train, hue=\"Cover_Type\")\n\n# Decoration\nplt.title(\"Box Plot of Horizontal_Distance_To_Fire_Points by Cover_Type\", fontsize=22, color=\"#006400\")\nplt.legend(title=\"Cover\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"11.  Attribute Wilderness_Area1 vs Cover_Type","metadata":{}},{"cell_type":"code","source":"# Draw Plot\nplt.figure(figsize=(16,10), dpi= 80)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 1, \"Wilderness_Area1\"], shade=True, color=\"#4169E1\", label=\"Cover=1\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 2, \"Wilderness_Area1\"], shade=True, color=\"#FF8C00\", label=\"Cover=2\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 3, \"Wilderness_Area1\"], shade=True, color=\"#FF4500\", label=\"Cover=3\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 4, \"Wilderness_Area1\"], shade=True, color=\"#BDB76B\", label=\"Cover=4\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 5, \"Wilderness_Area1\"], shade=True, color=\"#8B4513\", label=\"Cover=5\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 6, \"Wilderness_Area1\"], shade=True, color=\"#9400D3\", label=\"Cover=6\", alpha=.7)\nsns.kdeplot(train.loc[train[\"Cover_Type\"] == 7, \"Wilderness_Area1\"], shade=True, color=\"#006400\", label=\"Cover=7\", alpha=.7)\n\n# Decoration\nplt.title(\"The distribution of the attribute Wilderness_Area1\", color=\"#006400\", fontsize=22)\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Grouping Wilderness_Area and Soil_Type vs Cover_Type","metadata":{}},{"cell_type":"code","source":"cols = train.columns\n\n#number of rows=r , number of columns=c\nr,c = train.shape\n\n#Create a new dataframe with r rows, one column for each encoded category, and target in the end\ndata = pd.DataFrame(index=np.arange(0, r),columns=['Wilderness_Area','Soil_Type','Cover_Type'])\n\n#Make an entry in 'data' for each r as category_id, target value\nfor i in range(0,r):\n    w=0;\n    s=0;\n    # Category1 range\n    for j in range(10,14):\n        if (train.iloc[i,j] == 1):\n            w=j-9  #category class\n            break\n    # Category2 range        \n    for k in range(14,54):\n        if (train.iloc[i,k] == 1):\n            s=k-13 #category class\n            break\n    #Make an entry in 'data' for each r as category_id, target value        \n    data.iloc[i]=[w,s,train.iloc[i,c-1]]\n\n#Plot for Category1    \nsns.countplot(x=\"Wilderness_Area\", hue=\"Cover_Type\", data=data)\nplt.show()\n#Plot for Category2\nplt.rc(\"figure\", figsize=(25, 10))\nsns.countplot(x=\"Soil_Type\", hue=\"Cover_Type\", data=data)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Combine train and test sets","metadata":{}},{"cell_type":"code","source":"ntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train[\"Cover_Type\"].to_frame()\n\n#Combine train and test sets\nconcat_data = pd.concat((train, test), sort=False).reset_index(drop=True)\n#Drop the target \"Cover_Type\" and Id columns\nconcat_data.drop([\"Cover_Type\"], axis=1, inplace=True)\nconcat_data.drop([\"Id\"], axis=1, inplace=True)\nprint(\"Total size is :\",concat_data.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"concat_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"concat_data.tail()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"concat_data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Missing Values","metadata":{}},{"cell_type":"code","source":"# Count the null columns\nnull_columns = concat_data.columns[concat_data.isnull().any()]\nconcat_data[null_columns].isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*There are no missing values in this dataset. Let's go define numerical and categorical features.*","metadata":{}},{"cell_type":"code","source":"numeric_features = concat_data.select_dtypes(include=[np.number])\ncategoricals = concat_data.select_dtypes(exclude=[np.number])\n\nprint(f\"Numerical features: {numeric_features.shape}\")\nprint(f\"Categorical features: {categoricals.shape}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"concat_data.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we split the combined dataset to the original train and test sets\nTrainData = concat_data[:ntrain] \nTestData = concat_data[ntrain:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TrainData.shape, TestData.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TrainData.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TestData.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target = train[[\"Cover_Type\"]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"We make sure that both train and target sets have the same row number:\")\nprint(f\"Train: {TrainData.shape[0]} rows\")\nprint(f\"Target: {target.shape[0]} rows\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove any duplicated column names\nconcat_data = concat_data.loc[:,~concat_data.columns.duplicated()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = TrainData\ny = np.array(target)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n# Split the data set into train and test sets \nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train.shape, x_test.shape, y_train.shape, y_test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Scince we have outliers for scaling data using the mean and variance of the data is likely to not work very well. In this case, we can use robust_scale and RobustScaler as drop-in replacements instead.","metadata":{}},{"cell_type":"code","source":"scaler = RobustScaler()\n\n# transform \"x_train\"\nx_train = scaler.fit_transform(x_train)\n# transform \"x_test\"\nx_test = scaler.transform(x_test)\n#Transform the test set\nX_test= scaler.transform(TestData)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building Machine Learning Models\n1. Logistic Regression","metadata":{}},{"cell_type":"code","source":"# Baseline model of Logistic Regression with default parameters:\n\nlogistic_regression = linear_model.LogisticRegression()\nlogistic_regression_mod = logistic_regression.fit(x_train, y_train)\nprint(f\"Baseline Logistic Regression: {round(logistic_regression_mod.score(x_test, y_test), 3)}\")\n\npred_logistic_regression = logistic_regression_mod.predict(x_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> To improve this score(0.704), we want to search the set of \"hyperparameters\" by using common approach \"Grid search\". GridSearch exhaustively searches through all possible combinations of hyperparameters during training the phase. Before we proceed further, we shall cover other cross-validation (CV) methods since tuning hyperparameters via grid search is usually cross-validated to avoid overfitting. we are using the StratifiedKFold function with a stratified 3-fold (n_splits = 3) cross-validation.","metadata":{}},{"cell_type":"markdown","source":"* NOTE:For accelerating the running GridSearchCV we set: n-splits=3, n_jobs=2","metadata":{}},{"cell_type":"code","source":"cv_method = StratifiedKFold(n_splits=3, \n                            random_state=42\n                            )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cross validate Logistic Regression model\nscores_Logistic = cross_val_score(logistic_regression, x_train, y_train, cv =cv_method, n_jobs = 2, scoring = \"accuracy\")\n\nprint(f\"Scores(Cross validate) for Logistic Regression model:\\n{scores_Logistic}\")\nprint(f\"CrossValMeans: {round(scores_Logistic.mean(), 3)}\")\nprint(f\"CrossValStandard Deviation: {round(scores_Logistic.std(), 3)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params_LR = {\"tol\": [0.0001,0.0002,0.0003],\n            \"C\": [0.01, 0.1, 1, 10, 100],\n            \"intercept_scaling\": [1, 2, 3, 4]\n              }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"GridSearchCV_LR = GridSearchCV(estimator=linear_model.LogisticRegression(), \n                                param_grid=params_LR, \n                                cv=cv_method,\n                                verbose=1, \n                                n_jobs=2,\n                                scoring=\"accuracy\", \n                                return_train_score=True\n                                )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit model with train data\nGridSearchCV_LR.fit(x_train, y_train);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_estimator_LR = GridSearchCV_LR.best_estimator_\nprint(f\"Best estimator for LR model:\\n{best_estimator_LR}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_params_LR = GridSearchCV_LR.best_params_\nprint(f\"Best parameter values for LR model:\\n{best_params_LR}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Best score for LR model: {round(GridSearchCV_LR.best_score_, 3)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test with new parameter for LogisticRegression model\nlogistic_regression = linear_model.LogisticRegression(C=10, intercept_scaling=1, tol=0.0001, penalty=\"l2\", solver=\"liblinear\", random_state=42)\nlogistic_regression_mod = logistic_regression.fit(x_train, y_train)\npred_logistic_regression = logistic_regression_mod.predict(x_test)\n\nmse_logistic_regression = mean_squared_error(y_test, pred_logistic_regression)\nrmse_logistic_regression = np.sqrt(mean_squared_error(y_test, pred_logistic_regression))\nscore_logistic_regression_train = logistic_regression_mod.score(x_train, y_train)\nscore_logistic_regression_test = logistic_regression_mod.score(x_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Mean Square Error for Logistic Regression = {round(mse_logistic_regression, 3)}\")\nprint(f\"Root Mean Square Error for Logistic Regression = {round(rmse_logistic_regression, 3)}\")\nprint(f\"R^2(coefficient of determination) on training set = {round(score_logistic_regression_train, 3)}\")\nprint(f\"R^2(coefficient of determination) on testing set = {round(score_logistic_regression_test, 3)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Classification Report\")\nprint(classification_report(y_test, pred_logistic_regression))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, pred_logistic_regression))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2. K-Nearest Neighbors Classifier","metadata":{}},{"cell_type":"code","source":"# Baseline model of K-Nearest Neighbors with default parameters:\n\nknn = KNeighborsClassifier()\nknn_mod = knn.fit(x_train, y_train)\nprint(f\"Baseline K-Nearest Neighbors: {round(knn_mod.score(x_test, y_test), 3)}\")\n\npred_knn = knn_mod.predict(x_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cross validate K-Nearest Neighbors model\nscores_knn = cross_val_score(knn, x_train, y_train, cv = cv_method, n_jobs = 2, scoring = \"accuracy\")\n\nprint(f\"Scores(Cross validate) for K-Nearest Neighbors model:\\n{scores_knn}\")\nprint(f\"CrossValMeans: {round(scores_knn.mean(), 3)}\")\nprint(f\"CrossValStandard Deviation: {round(scores_knn.std(), 3)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params_knn = {\"leaf_size\": list(range(1,30)),\n              \"n_neighbors\": list(range(1,21)),\n              \"p\": [1,2]}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"GridSearchCV_knn = GridSearchCV(estimator=KNeighborsClassifier(), \n                                param_grid=params_knn, \n                                cv=cv_method,\n                                verbose=1, \n                                n_jobs=-1,\n                                scoring=\"accuracy\", \n                                return_train_score=True\n                                )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit model with train data\nGridSearchCV_knn.fit(x_train, y_train);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_estimator_knn = GridSearchCV_knn.best_estimator_\nprint(f\"Best estimator for KNN model:\\n{best_estimator_knn}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_params_knn = GridSearchCV_knn.best_params_\nprint(f\"Best parameter values:\\n{best_params_knn}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_score_knn = GridSearchCV_knn.best_score_\nprint(f\"Best score for GNB model: {round(best_score_knn, 3)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test with new parameter for KNN model\nknn = KNeighborsClassifier(leaf_size=1, n_neighbors=1 , p=1)\nknn_mod = knn.fit(x_train, y_train)\npred_knn = knn_mod.predict(x_test)\n\nmse_knn = mean_squared_error(y_test, pred_knn)\nrmse_knn = np.sqrt(mean_squared_error(y_test, pred_knn))\nscore_knn_train = knn_mod.score(x_train, y_train)\nscore_knn_test = knn_mod.score(x_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Mean Square Error for K_Nearest Neighbor  = {round(mse_knn, 3)}\")\nprint(f\"Root Mean Square Error for K_Nearest Neighbor = {round(rmse_knn, 3)}\")\nprint(f\"R^2(coefficient of determination) on training set = {round(score_knn_train, 3)}\")\nprint(f\"R^2(coefficient of determination) on testing set = {round(score_knn_test, 3)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Classification Report\")\nprint(classification_report(y_test, pred_knn))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, pred_knn))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"3. Gaussian Naive Bayes","metadata":{}},{"cell_type":"code","source":"gaussianNB = GaussianNB()\ngaussianNB_mod = gaussianNB.fit(x_train, y_train)\nprint(f\"Baseline Gaussin Navie Bayes: {round(gaussianNB_mod.score(x_test, y_test), 3)}\")\n\npred_gaussianNB = gaussianNB_mod.predict(x_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cross validate Gaussian Naive Bayes model\nscores_GNB = cross_val_score(gaussianNB, x_train, y_train, cv = cv_method, n_jobs = 2, scoring = \"accuracy\")\n\nprint(f\"Scores(Cross validate) for Gaussian Naive Bayes model:\\n{scores_GNB}\")\nprint(f\"CrossValMeans: {round(scores_GNB.mean(), 3)}\")\nprint(f\"CrossValStandard Deviation: {round(scores_GNB.std(), 3)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params_GNB = {\"C\": [0.1,0.25,0.5,1],\n              \"gamma\": [0.1,0.5,0.8,1.0],\n              \"kernel\": [\"rbf\",\"linear\"]}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"GridSearchCV_GNB = GridSearchCV(estimator=svm.SVC(), \n                                param_grid=params_GNB, \n                                cv=cv_method,\n                                verbose=1, \n                                n_jobs=-1,\n                                scoring=\"accuracy\", \n                                return_train_score=True\n                                )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit model with train data\nGridSearchCV_GNB.fit(x_train, y_train);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_estimator_GNB = GridSearchCV_GNB.best_estimator_\nprint(f\"Best estimator for DT model:\\n{best_estimator_GNB}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_params_GNB = GridSearchCV_GNB.best_params_\nprint(f\"Best parameter values:\\n{best_params_GNB}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_score_GNB = GridSearchCV_GNB.best_score_\nprint(f\"Best score for GNB model: {round(best_score_GNB, 3)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mse_gaussianNB = mean_squared_error(y_test, pred_gaussianNB)\nrmse_gaussianNB = np.sqrt(mean_squared_error(y_test, pred_gaussianNB))\nscore_gaussianNB_train = gaussianNB_mod.score(x_train, y_train)\nscore_gaussianNB_test = gaussianNB_mod.score(x_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Mean Square Error for Gaussian Naive Bayes = {round(mse_gaussianNB, 3)}\")\nprint(f\"Root Mean Square Error for Gaussian Naive Bayes = {round(rmse_gaussianNB, 3)}\")\nprint(f\"R^2(coefficient of determination) on training set = {round(score_gaussianNB_train, 3)}\")\nprint(f\"R^2(coefficient of determination) on testing set = {round(score_gaussianNB_test, 3)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Classification Report\")\nprint(classification_report(y_test, pred_gaussianNB))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, pred_gaussianNB))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"4. Support Vector Machine(SVM)","metadata":{}},{"cell_type":"code","source":"svc = SVC()\nsvc_mod = svc.fit(x_train, y_train)\nprint(f\"Baseline Support Vector Machine: {round(svc_mod.score(x_test, y_test), 3)}\")\n\npred_svc = svc_mod.predict(x_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cross validate SVC model\nscores_SVC = cross_val_score(svc, x_train, y_train, cv = cv_method, n_jobs = 2, scoring = \"accuracy\")\n\nprint(f\"Scores(Cross validate) for SVC model:\\n{scores_SVC}\")\nprint(f\"CrossValMeans: {round(scores_SVC.mean(), 3)}\")\nprint(f\"CrossValStandard Deviation: {round(scores_SVC.std(), 3)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params_SVC = {\"C\": [0.1, 1, 10, 100, 1000],  \n              \"gamma\": [1, 0.1, 0.01, 0.001, 0.0001], \n              \"kernel\": [\"rbf\"]\n              }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"GridSearchCV_SVC = GridSearchCV(estimator=SVC(), \n                                param_grid=params_SVC, \n                                cv=cv_method,\n                                verbose=1, \n                                n_jobs=-1,\n                                refit = True,\n                                scoring=\"accuracy\", \n                                return_train_score=True\n                                )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit model with train data\nGridSearchCV_SVC.fit(x_train, y_train);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_estimator_SVC = GridSearchCV_SVC.best_estimator_\nprint(f\"Best estimator for SVC model:\\n{best_estimator_SVC}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_params_SVC = GridSearchCV_SVC.best_params_\nprint(f\"Best parameter values:\\n{best_params_SVC}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_score_SVC = GridSearchCV_SVC.best_score_\nprint(f\"Best score for SVC model: {round(best_score_SVC, 3)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test with new parameter for SVC model\nsvc = SVC(C=100, gamma=0.1, kernel=\"rbf\" , random_state=42)\nsvc_mod = svc.fit(x_train, y_train)\npred_svc = svc_mod.predict(x_test)\n\nmse_svc = mean_squared_error(y_test, pred_svc)\nrmse_svc = np.sqrt(mean_squared_error(y_test, pred_svc))\nscore_svc_train = svc_mod.score(x_train, y_train)\nscore_svc_test = svc_mod.score(x_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Mean Square Error for Linear Support Vector Machine = {round(mse_svc, 3)}\")\nprint(f\"Root Mean Square Error for Linear Support Vector Machine = {round(rmse_svc, 3)}\")\nprint(f\"R^2(coefficient of determination) on training set = {round(score_svc_train, 3)}\")\nprint(f\"R^2(coefficient of determination) on testing set = {round(score_svc_test, 3)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Classification Report\")\nprint(classification_report(y_test, pred_svc))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, pred_svc))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"4. Decision Tree","metadata":{}},{"cell_type":"code","source":"decision_tree = DecisionTreeClassifier(random_state= 42)\ndecision_tree_mod = decision_tree.fit(x_train, y_train)\nprint(f\"Baseline Decision Tree: {round(decision_tree_mod.score(x_test, y_test), 3)}\")\n\npred_decision_tree = decision_tree_mod.predict(x_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cross validate Decision Tree model\nscores_DT = cross_val_score(decision_tree, x_train, y_train, cv = cv_method, n_jobs = 2, scoring = \"accuracy\")\n\nprint(f\"Scores(Cross validate) for Decision Tree model:\\n{scores_DT}\")\nprint(f\"CrossValMeans: {round(scores_DT.mean(), 3)}\")\nprint(f\"CrossValStandard Deviation: {round(scores_DT.std(), 3)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params_DT = {\"criterion\": [\"gini\", \"entropy\"],\n             \"max_depth\": [1, 2, 3, 4, 5, 6, 7, 8],\n             \"min_samples_split\": [2, 3]}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"GridSearchCV_DT = GridSearchCV(estimator=DecisionTreeClassifier(), \n                                param_grid=params_DT, \n                                cv=cv_method,\n                                verbose=1, \n                                n_jobs=-1,\n                                scoring=\"accuracy\", \n                                return_train_score=True\n                                )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit model with train data\nGridSearchCV_DT.fit(x_train, y_train);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_estimator_DT = GridSearchCV_DT.best_estimator_\nprint(f\"Best estimator for DT model:\\n{best_estimator_DT}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_params_DT = GridSearchCV_DT.best_params_\nprint(f\"Best parameter values:\\n{best_params_DT}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_score_DT = GridSearchCV_DT.best_score_\nprint(f\"Best score for DT model: {round(best_score_DT, 3)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"decision_tree = DecisionTreeClassifier(criterion=\"gini\", max_depth=8, min_impurity_split=2, min_samples_leaf=0.4, random_state=42)\ndecision_tree_mod = decision_tree.fit(x_train, y_train)\npred_decision_tree = decision_tree_mod.predict(x_test)\n\nmse_decision_tree = mean_squared_error(y_test, pred_decision_tree)\nrmse_decision_tree = np.sqrt(mean_squared_error(y_test, pred_decision_tree))\nscore_decision_tree_train = decision_tree_mod.score(x_train, y_train)\nscore_decision_tree_test = decision_tree_mod.score(x_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Mean Square Error for Decision Tree = {round(mse_decision_tree, 3)}\")\nprint(f\"Root Mean Square Error for Decision Tree = {round(rmse_decision_tree, 3)}\")\nprint(f\"R^2(coefficient of determination) on training set = {round(score_decision_tree_train, 3)}\")\nprint(f\"R^2(coefficient of determination) on testing set = {round(score_decision_tree_test, 3)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Classification Report\")\nprint(classification_report(y_test, pred_decision_tree))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, pred_decision_tree))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"5. Random Forest","metadata":{}},{"cell_type":"code","source":"random_forest = RandomForestClassifier()\nrandom_forest_mod = random_forest.fit(x_train, y_train)\nprint(f\"Baseline Random Forest: {round(random_forest_mod.score(x_test, y_test), 3)}\")\n\npred_random_forest = random_forest_mod.predict(x_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cross validate Random forest model\nscores_RF = cross_val_score(random_forest, x_train, y_train, cv = cv_method, n_jobs = 2, scoring = \"accuracy\")\n\nprint(f\"Scores(Cross validate) for Random forest model:\\n{scores_RF}\")\nprint(f\"CrossValMeans: {round(scores_RF.mean(), 3)}\")\nprint(f\"CrossValStandard Deviation: {round(scores_RF.std(), 3)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params_RF = {\"min_samples_split\": [2, 6, 20],\n              \"min_samples_leaf\": [1, 4, 16],\n              \"n_estimators\" :[100,200,300,400],\n              \"criterion\": [\"gini\"]             \n              }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"GridSearchCV_RF = GridSearchCV(estimator=RandomForestClassifier(), \n                                param_grid=params_RF, \n                                cv=cv_method,\n                                verbose=1, \n                                n_jobs=2,\n                                scoring=\"accuracy\", \n                                return_train_score=True\n                                )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit model with train data\nGridSearchCV_RF.fit(x_train, y_train);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_estimator_RF = GridSearchCV_RF.best_estimator_\nprint(f\"Best estimator for RF model:\\n{best_estimator_RF}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_params_RF = GridSearchCV_RF.best_params_\nprint(f\"Best parameter values for RF model:\\n{best_params_RF}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_score_RF = GridSearchCV_RF.best_score_\nprint(f\"Best score for RF model: {round(best_score_RF, 3)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_forest = RandomForestClassifier(criterion=\"gini\", n_estimators=400, min_samples_leaf=1, min_samples_split=2, random_state=42)\nrandom_forest_mod = random_forest.fit(x_train, y_train)\npred_random_forest = random_forest_mod.predict(x_test)\n\nmse_random_forest = mean_squared_error(y_test, pred_random_forest)\nrmse_random_forest = np.sqrt(mean_squared_error(y_test, pred_random_forest))\nscore_random_forest_train = random_forest_mod.score(x_train, y_train)\nscore_random_forest_test = random_forest_mod.score(x_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Mean Square Error for Random Forest = {round(mse_random_forest, 3)}\")\nprint(f\"Root Mean Square Error for Random Forest = {round(rmse_random_forest, 3)}\")\nprint(f\"R^2(coefficient of determination) on training set = {round(score_random_forest_train, 3)}\")\nprint(f\"R^2(coefficient of determination) on testing set = {round(score_random_forest_test, 3)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Classification Report\")\nprint(classification_report(y_test, pred_random_forest))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, pred_random_forest))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Next step, we'll ckeck the important features that our model used to make predictions.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(16,10))\nviz = FeatureImportances(random_forest)\nviz.fit(x_train, y_train)\nviz.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plotting the learning curve","metadata":{}},{"cell_type":"code","source":"# Plot learning curve\ndef plot_learning_curve(estimator, title, x, y, ylim=None, cv=None,\n                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n        \n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, x, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"#80CBC4\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"#00897B\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Logistic Regression\nplot_learning_curve(GridSearchCV_LR.best_estimator_,title = \"Logistic Regressionr learning curve\", x = x_train, y = y_train, cv = cv_method);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# KNN Classifier\nplot_learning_curve(GridSearchCV_knn.best_estimator_,title = \"KNN Classifier learning curve\", x = x_train, y = y_train, cv = cv_method);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Gaussian Naive Bayes\nplot_learning_curve(GridSearchCV_GNB.best_estimator_,title = \"Gaussian Naive Bayes learning curve\", x = x_train, y = y_train, cv = cv_method);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Support Vector Machine(SVM)\nplot_learning_curve(GridSearchCV_SVC.best_estimator_,title = \"Support Vector Machine(SVM) learning curve\", x = x_train, y = y_train, cv = cv_method);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Decision Tree\nplot_learning_curve(GridSearchCV_DT.best_estimator_,title = \"Decision Tree learning curve\", x = x_train, y = y_train, cv = cv_method);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Random Forest\nplot_learning_curve(GridSearchCV_RF.best_estimator_,title = \"Random Forest learning curve\", x = x_train, y = y_train, cv = cv_method);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Which is the best Model ?","metadata":{}},{"cell_type":"code","source":"results = pd.DataFrame({\n                        \"Model\": [\"Logistic Regression\",\n                                    \"KNN Classifier\",\n                                    \"Gaussian Naive Bayes\",\n                                    \"Support Vector Machine(SVM)\",\n                                    \"Decision Tree\",\n                                    \"Random Forest\"],\n                        \"Score\": [logistic_regression_mod.score(x_train, y_train),\n                                    knn_mod.score(x_train, y_train),\n                                    gaussianNB_mod.score(x_train, y_train),\n                                    svc_mod.score(x_train, y_train),\n                                    decision_tree_mod.score(x_train, y_train),\n                                    random_forest_mod.score(x_train, y_train)]\n                        })\nresult_df = results.sort_values(by=\"Score\", ascending=False)\nresult_df = result_df.set_index(\"Score\")\nresult_df.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ENSEMBLE METHODS","metadata":{}},{"cell_type":"code","source":"vote = VotingClassifier([(\"Random Forest\", random_forest_mod), (\"KNN Classifier\", knn_mod)])\nvote_mod = vote.fit(x_train, y_train.ravel())\nvote_pred = vote_mod.predict(x_test)\n\nprint(f\"Root Mean Square Error test for ENSEMBLE METHODS: {round(np.sqrt(mean_squared_error(y_test, vote_pred)), 3)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Submission to Kaggle","metadata":{}},{"cell_type":"code","source":"test[\"Id\"].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Final_Submission_ForestCoverType = pd.DataFrame({\n        \"Id\": test[\"Id\"],\n        \"Cover_Type\": vote_mod.predict(X_test)})\n\nFinal_Submission_ForestCoverType.to_csv(\"Final_Submission_ForestCoverType.csv\", index=False)\nFinal_Submission_ForestCoverType.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Thank you for taking the time to read through my kernel.\nFor the moment, let me know if you found this notebook useful or you just liked it: I would really appreciate it!*","metadata":{}}]}
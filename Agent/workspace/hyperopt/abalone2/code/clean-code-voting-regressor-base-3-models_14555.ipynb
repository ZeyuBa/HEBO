{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":72489,"databundleVersionId":8096274,"sourceType":"competition"},{"sourceId":57419,"sourceType":"datasetVersion","datasetId":37691},{"sourceId":172117662,"sourceType":"kernelVersion"}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"papermill":{"default_parameters":{},"duration":267.209901,"end_time":"2024-04-02T10:52:26.992053","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-04-02T10:47:59.782152","version":"2.5.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"I updated params of models from here : https://www.kaggle.com/code/igorvolianiuk/abalone-rings-ensemble/notebook\n\nThen use best feature that finded from here : https://www.kaggle.com/code/mfmfmf3/feature-selection-base-genetic-algorithm\n\n\nMaking an ensemble may improve our LB (Leaderboard) score, but it may, we fail in the private score :(\n\nFor making ensemble i use the result of this notebook:\nhttps://www.kaggle.com/code/arunklenin/ps4e4-feature-engineering-regression","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -U scikit-learn -q","metadata":{"execution":{"iopub.status.busy":"2024-04-08T01:39:55.241752Z","iopub.execute_input":"2024-04-08T01:39:55.242028Z","iopub.status.idle":"2024-04-08T01:40:12.22952Z","shell.execute_reply.started":"2024-04-08T01:39:55.242005Z","shell.execute_reply":"2024-04-08T01:40:12.228589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\nfrom sklearn.metrics import make_scorer, mean_squared_error, root_mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.ensemble import VotingRegressor\nfrom catboost import CatBoostRegressor\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nimport matplotlib.pyplot as plt\nfrom sklearn.base import copy\nimport pandas as pd\nimport numpy as np\nimport random\nimport optuna","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":5.122272,"end_time":"2024-04-02T10:48:07.678245","exception":false,"start_time":"2024-04-02T10:48:02.555973","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-08T01:40:12.231611Z","iopub.execute_input":"2024-04-08T01:40:12.231894Z","iopub.status.idle":"2024-04-08T01:40:17.150809Z","shell.execute_reply.started":"2024-04-08T01:40:12.231869Z","shell.execute_reply":"2024-04-08T01:40:17.149832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"RANDOM_SEED = 42\n\n# This action may be dangerous for the private score\nMAKING_ENSEMBLE = True\n\nFIND_BEST_PARAMS = False\nAPPLY_LOG_TRANSFORMATION = True\nAPPLY_FEATURE_ENGINEERING = True\n\nnp.random.seed(RANDOM_SEED)\nrandom.seed(RANDOM_SEED)","metadata":{"execution":{"iopub.status.busy":"2024-04-08T01:40:17.152113Z","iopub.execute_input":"2024-04-08T01:40:17.152609Z","iopub.status.idle":"2024-04-08T01:40:17.157672Z","shell.execute_reply.started":"2024-04-08T01:40:17.152574Z","shell.execute_reply":"2024-04-08T01:40:17.156624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train   = pd.read_csv('/kaggle/input/playground-series-s4e4/train.csv')\norginal = pd.read_csv('/kaggle/input/abalone-dataset/abalone.csv')\ntest    = pd.read_csv('/kaggle/input/playground-series-s4e4/test.csv')","metadata":{"execution":{"iopub.status.busy":"2024-04-08T01:40:17.160056Z","iopub.execute_input":"2024-04-08T01:40:17.160346Z","iopub.status.idle":"2024-04-08T01:40:17.443045Z","shell.execute_reply.started":"2024-04-08T01:40:17.160321Z","shell.execute_reply":"2024-04-08T01:40:17.442064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.drop(['id'], axis = 1)\ntrain.columns = orginal.columns\ntrain = pd.concat([train, orginal], axis = 0, ignore_index=True)\n\ny = train['Rings']\n# Because RMSLE score, We make a conversion like below:\ny_log = np.log(1+y)\n# Add the end for getting the result we back to original like below:\n# y = np.exp(y_log)-1\n\n\ntrain = train.drop(['Rings'], axis = 1)\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-08T01:40:17.444165Z","iopub.execute_input":"2024-04-08T01:40:17.444472Z","iopub.status.idle":"2024-04-08T01:40:17.483242Z","shell.execute_reply.started":"2024-04-08T01:40:17.444448Z","shell.execute_reply":"2024-04-08T01:40:17.482362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_id = test['id']\ntest = test.drop('id', axis = 1)\ntest.columns = train.columns\ntest.head()","metadata":{"papermill":{"duration":0.279287,"end_time":"2024-04-02T10:48:07.97413","exception":false,"start_time":"2024-04-02T10:48:07.694843","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-08T01:40:17.48441Z","iopub.execute_input":"2024-04-08T01:40:17.484723Z","iopub.status.idle":"2024-04-08T01:40:17.502264Z","shell.execute_reply.started":"2024-04-08T01:40:17.4847Z","shell.execute_reply":"2024-04-08T01:40:17.501234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder = OneHotEncoder(sparse_output = False, handle_unknown = 'ignore')\n\ntrain = pd.concat([\n                    train.iloc[:,1:], \n                    pd.DataFrame(encoder.fit_transform(train[['Sex']]).astype('int'), \n                                 columns = encoder.categories_[0])\n                    ], \n                    axis = 1\n                )\n\ntest  = pd.concat([\n                    test.iloc[:,1:], \n                    pd.DataFrame(encoder.transform(test[['Sex']]).astype('int'), \n                                 columns = encoder.categories_[0])\n                    ], \n                    axis = 1\n                )","metadata":{"papermill":{"duration":0.075281,"end_time":"2024-04-02T10:48:08.512815","exception":false,"start_time":"2024-04-02T10:48:08.437534","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-08T01:40:17.503408Z","iopub.execute_input":"2024-04-08T01:40:17.503672Z","iopub.status.idle":"2024-04-08T01:40:17.569036Z","shell.execute_reply.started":"2024-04-08T01:40:17.50365Z","shell.execute_reply":"2024-04-08T01:40:17.568322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def log_transformation(data, columns):\n    for column in columns:\n        positive_values = data[column] - data[column].min() + 1\n        data[f'{column}_log'] = np.log(positive_values)\n    return data\n\n\nif APPLY_LOG_TRANSFORMATION:\n    train = log_transformation(train, ['Length', 'Diameter', 'Height', 'Whole weight', 'Shucked weight','Viscera weight', 'Shell weight'])\n    test  = log_transformation(test, ['Length', 'Diameter', 'Height', 'Whole weight', 'Shucked weight','Viscera weight', 'Shell weight'])","metadata":{"execution":{"iopub.status.busy":"2024-04-08T01:40:17.570284Z","iopub.execute_input":"2024-04-08T01:40:17.570896Z","iopub.status.idle":"2024-04-08T01:40:17.597908Z","shell.execute_reply.started":"2024-04-08T01:40:17.570843Z","shell.execute_reply":"2024-04-08T01:40:17.597079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n\ndef objective(trial):\n\n    params = {\n        \"verbose\": False,\n        \"iterations\": 1000,\n        \"loss_function\":'RMSE',\n        \"random_state\": RANDOM_SEED,\n        \"depth\": trial.suggest_int(\"depth\", 3, 15),\n        \"subsample\": trial.suggest_float(\"subsample\", 0.01, 1.0),\n        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 1, 100),\n        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 1.0),\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.1, log=True),\n    }\n\n    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n    scores = []\n    for _, (train_index, valid_index) in enumerate(cv.split(train, y)):\n        X_train, y_train = train.iloc[train_index], y_log.iloc[train_index]\n        X_valid, y_valid = train.iloc[valid_index], y_log.iloc[valid_index]\n        model = CatBoostRegressor(**params)\n\n        model.fit(X_train, y_train, \n                  eval_set=(X_valid, y_valid),\n                  early_stopping_rounds=100)\n        \n        y_pred = model.predict(X_valid)\n        scores.append(root_mean_squared_error(y_valid, y_pred))\n    return np.mean(scores)\n\n\nstudy = optuna.create_study(direction='minimize', study_name=\"optuna_catboost\")\nif FIND_BEST_PARAMS:\n    study.optimize(objective, n_trials=50)\n    print(f\"Best trial average RMSE: {study.best_value:.4f}\")\n    for key, value in study.best_params.items():\n        print(f\"{key}: {value}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-08T01:40:17.598867Z","iopub.execute_input":"2024-04-08T01:40:17.599123Z","iopub.status.idle":"2024-04-08T01:40:17.610319Z","shell.execute_reply.started":"2024-04-08T01:40:17.599101Z","shell.execute_reply":"2024-04-08T01:40:17.609456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n\ndef objective(trial):\n\n    params = {\n        'n_jobs':-1,\n        \"metric\":'rmse',  \n        \"verbosity\": -1,\n        \"bagging_freq\": 1,\n        \"boosting_type\": \"gbdt\",    \n        \"objective\":'regression', \n        'random_state':RANDOM_SEED,\n        'max_depth': trial.suggest_int('max_depth', 3, 15),                        \n        \"subsample\": trial.suggest_float(\"subsample\", 0.05, 1.0),\n        \"n_estimators\": trial.suggest_int('n_estimators', 400, 1000),\n        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 1, 100),               \n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.005, 0.01),\n        'min_child_samples': trial.suggest_int('min_child_samples', 10, 60),\n        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.3, 1.0),\n    }\n\n    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n    scores = []\n    for _, (train_index, valid_index) in enumerate(cv.split(train, y)):\n        X_train, y_train = train.iloc[train_index], y_log.iloc[train_index]\n        X_valid, y_valid = train.iloc[valid_index], y_log.iloc[valid_index]\n        model = LGBMRegressor(**params)\n        model.fit(X_train, y_train)\n        y_pred = model.predict(X_valid)     \n        scores.append(root_mean_squared_error(y_valid, y_pred))\n    return np.mean(scores)\n\n\nstudy = optuna.create_study(direction='minimize', study_name=\"optuna_lgbm\")\nif FIND_BEST_PARAMS:\n    study.optimize(objective, n_trials=50)\n    print(f\"Best trial average RMSE: {study.best_value:.4f}\")\n    for key, value in study.best_params.items():\n        print(f\"{key}: {value}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-08T01:40:17.613147Z","iopub.execute_input":"2024-04-08T01:40:17.613466Z","iopub.status.idle":"2024-04-08T01:40:17.629116Z","shell.execute_reply.started":"2024-04-08T01:40:17.613437Z","shell.execute_reply":"2024-04-08T01:40:17.628177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n\ndef objective(trial):\n\n    params = {\n        'eval_metric': 'rmse',\n        'random_state': RANDOM_SEED,\n        'objective': 'reg:squarederror',\n        'gamma': trial.suggest_float(\"gamma\", 1e-2, 1.0),\n        'max_depth': trial.suggest_int('max_depth',2, 20),\n        'subsample': trial.suggest_float(\"subsample\", 0.05, 1.0),\n        'n_estimators': trial.suggest_int('n_estimators',100, 1000),\n        'min_child_weight': trial.suggest_int('min_child_weight',2, 20),\n        'colsample_bytree': trial.suggest_float(\"colsample_bytree\", 0.05, 1.0),\n        'learning_rate': trial.suggest_float(\"learning_rate\", 1e-3, 0.1, log=True),\n    }\n\n    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n    scores = []\n    for _, (train_index, valid_index) in enumerate(cv.split(train, y)):\n        X_train, y_train = train.iloc[train_index], y_log.iloc[train_index]\n        X_valid, y_valid = train.iloc[valid_index], y_log.iloc[valid_index]\n        model = XGBRegressor(**params)\n        model.fit(X_train, y_train)\n        y_pred = model.predict(X_valid)     \n        scores.append(root_mean_squared_error(y_valid, y_pred))\n    return np.mean(scores)\n\n\nstudy = optuna.create_study(direction='minimize', study_name=\"optuna_xgboost\")\nif FIND_BEST_PARAMS:\n    study.optimize(objective, n_trials=50)\n    print(f\"Best trial average RMSE: {study.best_value:.4f}\")\n    for key, value in study.best_params.items():\n        print(f\"{key}: {value}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-08T01:40:17.630312Z","iopub.execute_input":"2024-04-08T01:40:17.630951Z","iopub.status.idle":"2024-04-08T01:40:17.647121Z","shell.execute_reply.started":"2024-04-08T01:40:17.630925Z","shell.execute_reply":"2024-04-08T01:40:17.646277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgboost_params = {\n    'max_depth': 10, \n    'verbosity': 0,\n    'random_state':RANDOM_SEED,\n    'device': 'cuda',\n    'booster': 'gbtree',\n    'n_estimators': 1137, \n    'tree_method': 'hist',\n    'min_child_weight': 7, \n    'grow_policy': 'lossguide', \n    'gamma': 0.03816426816838989, \n    'subsample': 0.486382907668344, \n    'objective': 'reg:squarederror',\n    'reg_lambda': 1.7487237399420372, \n    'reg_alpha': 0.013043045359306716,\n    'learning_rate': 0.011733966748427322, \n    'colsample_bytree': 0.5748511749872887, \n}\n\nlgbm_params = {\n     'metric':'rmse', \n     'device':'gpu', \n     'verbosity': -1,\n     'max_depth': 15,\n     'random_state':RANDOM_SEED,\n     'num_leaves': 138, \n     'n_estimators': 913, \n     'boosting_type': 'gbdt', \n     'min_child_samples': 34, \n     'objective':'regression', \n     'subsample_for_bin': 185680, \n     'subsample': 0.799314727120346, \n     'reg_alpha': 5.916235901972299e-09, \n     'reg_lambda': 6.943912907338958e-08, \n     'learning_rate': 0.01851440025520457, \n     'colsample_bytree': 0.4339090795122026, \n}\n\ncatboost_params = {\n    'depth': 15, \n    'max_bin': 464, \n    'verbose': False,\n    'random_state':RANDOM_SEED,\n    'task_type': 'CPU', \n    'eval_metric': 'RMSE', \n    'min_data_in_leaf': 78, \n    'loss_function': 'RMSE', \n    'grow_policy': 'Lossguide', \n    'bootstrap_type': 'Bernoulli', \n    'subsample': 0.83862137638162, \n    'l2_leaf_reg': 8.365422739510098, \n    'random_strength': 3.296124856352495, \n    'learning_rate': 0.09992185242598203, \n}","metadata":{"execution":{"iopub.status.busy":"2024-04-08T01:40:17.648152Z","iopub.execute_input":"2024-04-08T01:40:17.648431Z","iopub.status.idle":"2024-04-08T01:40:17.65971Z","shell.execute_reply.started":"2024-04-08T01:40:17.648402Z","shell.execute_reply":"2024-04-08T01:40:17.658969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv_estimators = [\n    ('lgbm', LGBMRegressor(**lgbm_params)),\n    ('xgboost', XGBRegressor(**xgboost_params)),\n    ('catboost', CatBoostRegressor(**catboost_params))\n]","metadata":{"execution":{"iopub.status.busy":"2024-04-08T01:40:17.660836Z","iopub.execute_input":"2024-04-08T01:40:17.661132Z","iopub.status.idle":"2024-04-08T01:40:17.674047Z","shell.execute_reply.started":"2024-04-08T01:40:17.661087Z","shell.execute_reply":"2024-04-08T01:40:17.673248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n\ndef objective(trial):\n    \n    params = {\n        'lgbm_weight': trial.suggest_float('lgbm_weight', 0.0, 5.0),\n        'xgboost_weight': trial.suggest_float('xgboost_weight', 0.0, 5.0),\n        'catboost_weight': trial.suggest_float('catboost_weight', 0.0, 5.0),\n    }\n\n\n    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n    scores = []\n    for _, (train_index, valid_index) in enumerate(cv.split(train, y)):\n        X_train, y_train = train.iloc[train_index], y_log.iloc[train_index]\n        X_valid, y_valid = train.iloc[valid_index], y_log.iloc[valid_index]\n        voting_regressor = VotingRegressor(\n            estimators=cv_estimators,\n            weights=[params['lgbm_weight'], params['xgboost_weight'], params['catboost_weight']]\n        )\n        voting_regressor.fit(X_train, y_train)\n        y_pred = voting_regressor.predict(X_valid)  \n        scores.append(root_mean_squared_error(y_valid, y_pred))\n    return np.mean(scores)\n\n\nstudy = optuna.create_study(direction='minimize', study_name=\"voting_regressor_optuna\")\nif FIND_BEST_PARAMS:\n    study.optimize(objective, n_trials=100)\n    print(f\"Best trial average RMSE: {study.best_value:.4f}\")\n    for key, value in study.best_params.items():\n        print(f\"{key}: {value}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-08T01:40:17.675097Z","iopub.execute_input":"2024-04-08T01:40:17.675423Z","iopub.status.idle":"2024-04-08T01:40:17.688574Z","shell.execute_reply.started":"2024-04-08T01:40:17.675377Z","shell.execute_reply":"2024-04-08T01:40:17.687785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\ntrain2 = train.copy()\ntest2  = test.copy()\n\n# I find these drop cols with feature selection base genetic algorithm\nlst_drop_cols = [\n    ['Shucked weight', 'Shell weight', 'Length_log', 'Diameter_log', 'Height_log', 'Viscera weight_log'],\n                 ['Shell weight', 'I', 'Length_log', 'Height_log', 'Viscera weight_log']]\n\nlst_y_pred_test = []\nfor i in range(len(lst_drop_cols)):\n    if APPLY_FEATURE_ENGINEERING:\n        train2 = train.drop(lst_drop_cols[i], axis=1)\n        test2  = test.drop(lst_drop_cols[i], axis=1)\n\n    weight_best_params = {\n        'lgbm_weight': 4.104966149239676, \n        'xgboost_weight': 0.48550637896530635, \n        'catboost_weight': 4.189724537494019,\n    }\n\n\n    voting_regressor = VotingRegressor(\n        estimators=cv_estimators,\n        weights=[ weight_best_params['lgbm_weight'], \n                  weight_best_params['xgboost_weight'], \n                  weight_best_params['catboost_weight']\n        ]\n    )\n\n    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n    scores = []\n    y_pred_test = []\n    for fold_i, (train_index, valid_index) in enumerate(cv.split(train, y)):\n        X_train, y_train = train2.iloc[train_index], y_log.iloc[train_index]\n        X_valid, y_valid = train2.iloc[valid_index], y_log.iloc[valid_index]\n        voting_regressor.fit(X_train, y_train)\n        y_pred = voting_regressor.predict(X_valid)  \n        score = root_mean_squared_error(y_valid, y_pred)\n        scores.append(score)\n        y_pred_test.append(voting_regressor.predict(test2))\n        print(f\"FOLD {fold_i} Done. RMSE : {score}\")\n    print(f\"All FOLD. Mean RMSE : {np.mean(scores)}\")\n    lst_y_pred_test.append(np.mean(y_pred_test, axis=0))","metadata":{"execution":{"iopub.status.busy":"2024-04-08T01:40:35.195593Z","iopub.execute_input":"2024-04-08T01:40:35.195942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = np.mean(lst_y_pred_test, axis=0)\nsub  = pd.DataFrame(columns = ['id', 'Rings'])\nsub['id'] = test_id\nsub['Rings'] = np.exp(predictions)-1\nsub.to_csv('submission_0.14550.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2024-04-08T01:40:18.16713Z","iopub.execute_input":"2024-04-08T01:40:18.167476Z","iopub.status.idle":"2024-04-08T01:40:18.265508Z","shell.execute_reply.started":"2024-04-08T01:40:18.167449Z","shell.execute_reply":"2024-04-08T01:40:18.264618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ndf1 = pd.read_csv('/kaggle/input/ps4e4-feature-engineering-regression/submission_comb.csv')\ndf2 = sub","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if MAKING_ENSEMBLE:\n    zr1 = 0.9\n    zr2 = 0.1\n\n    df3 = df1.copy()\n    df3['Rings'] = df1['Rings']*zr1 + df2['Rings']*zr2\n    df3.to_csv('submission.csv', index=False)\n\nelse:\n    sub.to_csv('submission.csv', index = False)","metadata":{},"execution_count":null,"outputs":[]}]}
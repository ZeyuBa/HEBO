{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":68479,"databundleVersionId":7609535,"sourceType":"competition"},{"sourceId":7009925,"sourceType":"datasetVersion","datasetId":4030196}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Multi-Class Prediction of Obesity ðŸ¤°ðŸ«ƒ Risk (Beginners/EasyðŸ§¿)\n\nIf you like my work please do consider following me ;)<br>\n[Divyam6969's Kaggle profile](www.kaggle.com/divyam6969)<br>\n[Divyam6969's Github profile](www.github.com/divyam6969)\n\n\n## Introduction\nThis project focuses on predicting the risk of obesity using a machine learning model. The solution presented here utilizes LightGBM (LGBM) combined with Optuna for hyperparameter optimization. It is part of the \"Playground Series - Season 4, Episode 2\" by Aryangupta30, available on Kaggle.\n\n## Description\nHyperparameter tuning plays a crucial role in enhancing the performance of machine learning models. This project demonstrates the effectiveness of using Optuna to systematically search for the best set of hyperparameters for the LGBM model.\n\n### Methodology\nThe Optuna module is employed to search through a predefined set of hyperparameters and a specified number of random parameter combinations. The hyperparameters considered for tuning include [list of hyperparameters].\n\n### Impact on Model Performance\nAfter hyperparameter tuning, the model's accuracy increased to 91.943%, representing a 0.903% improvement over the previous performance. This enhancement underscores the importance of optimizing hyperparameters to fine-tune the model's behavior and improve its predictive power.\n\n## Conclusion\nIncorporating hyperparameter tuning has resulted in a substantial improvement in model performance, as evidenced by the increase in accuracy. This highlights the significance of fine-tuning model parameters to achieve optimal results in machine learning tasks.\n\n## Notebooks and Code\nThe project notebooks can be found on Kaggle at [Divyam6969's profile](www.kaggle.com/divyam6969).\n\n## Code Overview\nThe project involves the following key steps:\n\n1. Importing necessary libraries including pandas, seaborn, matplotlib, numpy, scikit-learn, LightGBM, and Optuna.\n2. Loading and initial inspection of the datasets including train_data, test_data, and original_data.\n3. Visualizing null values in the datasets (there are no null values).\n4. Extracting variable types (continuous and categorical variables).\n5. Plotting distribution of categorical columns.\n6. Plotting histograms and density plots for continuous variables.\n7. Utilizing Optuna for hyperparameter optimization.\n8. Adjusting hyperparameters based on performance evaluation.\n9. Creating an instance of LGBMClassifier with the best parameters.\n10. Making predictions on the test set and generating a submission file.\n\n## Final Submission\nThe final predictions are made using the optimized model and submitted to the competition. The submission achieves a high accuracy score.\n\n## Additional Information\nFurther optimization of the model can be explored by adjusting additional hyperparameters. If you found this notebook helpful, please consider upvoting it.\n\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-06T15:00:55.529487Z","iopub.execute_input":"2024-02-06T15:00:55.529898Z","iopub.status.idle":"2024-02-06T15:00:55.974841Z","shell.execute_reply.started":"2024-02-06T15:00:55.529868Z","shell.execute_reply":"2024-02-06T15:00:55.973567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Importing scikit-learn modules\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n\n# Importing LGBMClassifier\nfrom lightgbm import LGBMClassifier\n\n# Importing Optuna for hyperparameter optimization\nfrom optuna.samplers import TPESampler\nimport optuna\n\n# Ignoring warnings for cleaner output\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Pandas setting to display more dataset rows and columns\npd.set_option('display.max_rows', 150)\npd.set_option('display.max_columns', 600)\n\n\n# Set Seaborn style\nsns.set(style=\"whitegrid\")","metadata":{"execution":{"iopub.status.busy":"2024-02-06T15:01:16.753568Z","iopub.execute_input":"2024-02-06T15:01:16.754276Z","iopub.status.idle":"2024-02-06T15:01:16.762414Z","shell.execute_reply.started":"2024-02-06T15:01:16.75424Z","shell.execute_reply":"2024-02-06T15:01:16.761444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/input/playground-series-s4e2/train.csv')\ntrain_data.name = \"Train Dataset\"\ntest_data = pd.read_csv('/kaggle/input/playground-series-s4e2/test.csv')\nsample_submission = pd.read_csv('/kaggle/input/playground-series-s4e2/sample_submission.csv')\n\noriginal_data = pd.read_csv('/kaggle/input/obesity-or-cvd-risk-classifyregressorcluster/ObesityDataSet.csv')\noriginal_data.name = \"Original Dataset\"\n\nprint(\"# Train Data INFO\\n\")\nprint(train_data.info())\nprint('='*50)\nprint(\"\\n# Original Data INFO\\n\")\nprint(train_data.info())","metadata":{"execution":{"iopub.status.busy":"2024-02-06T15:01:21.120148Z","iopub.execute_input":"2024-02-06T15:01:21.121365Z","iopub.status.idle":"2024-02-06T15:01:21.342989Z","shell.execute_reply.started":"2024-02-06T15:01:21.121326Z","shell.execute_reply":"2024-02-06T15:01:21.341859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-02-06T15:01:26.284193Z","iopub.execute_input":"2024-02-06T15:01:26.284603Z","iopub.status.idle":"2024-02-06T15:01:26.312376Z","shell.execute_reply.started":"2024-02-06T15:01:26.284569Z","shell.execute_reply":"2024-02-06T15:01:26.311085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.describe().T.style.background_gradient()","metadata":{"execution":{"iopub.status.busy":"2024-02-06T15:01:30.198943Z","iopub.execute_input":"2024-02-06T15:01:30.19938Z","iopub.status.idle":"2024-02-06T15:01:30.31227Z","shell.execute_reply.started":"2024-02-06T15:01:30.199346Z","shell.execute_reply":"2024-02-06T15:01:30.311126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-02-06T15:01:35.53978Z","iopub.execute_input":"2024-02-06T15:01:35.540316Z","iopub.status.idle":"2024-02-06T15:01:35.571321Z","shell.execute_reply.started":"2024-02-06T15:01:35.540275Z","shell.execute_reply":"2024-02-06T15:01:35.570153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.describe().T.style.background_gradient()","metadata":{"execution":{"iopub.status.busy":"2024-02-06T15:01:39.03906Z","iopub.execute_input":"2024-02-06T15:01:39.039468Z","iopub.status.idle":"2024-02-06T15:01:39.093917Z","shell.execute_reply.started":"2024-02-06T15:01:39.039438Z","shell.execute_reply":"2024-02-06T15:01:39.092794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"original_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-02-06T15:01:43.780932Z","iopub.execute_input":"2024-02-06T15:01:43.78136Z","iopub.status.idle":"2024-02-06T15:01:43.806802Z","shell.execute_reply.started":"2024-02-06T15:01:43.781328Z","shell.execute_reply":"2024-02-06T15:01:43.805378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"original_data.describe().T.style.background_gradient()","metadata":{"execution":{"iopub.status.busy":"2024-02-06T15:01:47.428547Z","iopub.execute_input":"2024-02-06T15:01:47.428928Z","iopub.status.idle":"2024-02-06T15:01:47.474149Z","shell.execute_reply.started":"2024-02-06T15:01:47.428899Z","shell.execute_reply":"2024-02-06T15:01:47.472981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a subplot with dimensions (1, 3)\nfig, axes = plt.subplots(1, 3, figsize=(18, 6))\n\n# Visualize null values in train dataset\nsns.heatmap(train_data.isna(), cmap='gray', cbar=False, ax=axes[0])\naxes[0].set_title('Train Dataset')\n\n# Visualize null values in test dataset\nsns.heatmap(test_data.isna(), cmap='gray', cbar=False, ax=axes[1])\naxes[1].set_title('Test Dataset')\n\n# Visualize null values in original dataset\nsns.heatmap(original_data.isna(), cmap='gray', cbar=False, ax=axes[2])\naxes[2].set_title('Original Dataset')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-06T15:01:51.230796Z","iopub.execute_input":"2024-02-06T15:01:51.231336Z","iopub.status.idle":"2024-02-06T15:01:54.915207Z","shell.execute_reply.started":"2024-02-06T15:01:51.231291Z","shell.execute_reply":"2024-02-06T15:01:54.913903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_variable_types(dataframe):\n    continuous_vars = []\n    categorical_vars = []\n\n    for column in dataframe.columns:\n        if dataframe[column].dtype == 'object':\n            categorical_vars.append(column)\n        else:\n            continuous_vars.append(column)\n\n    return continuous_vars, categorical_vars\n\ncontinuous_vars, categorical_vars = get_variable_types(train_data)\ncontinuous_vars.remove('id'), categorical_vars.remove('NObeyesdad')\n\nprint(\"Continuous Variables:\", continuous_vars)\nprint(\"Categorical Variables:\", categorical_vars)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-06T15:01:57.900887Z","iopub.execute_input":"2024-02-06T15:01:57.90142Z","iopub.status.idle":"2024-02-06T15:01:57.91214Z","shell.execute_reply.started":"2024-02-06T15:01:57.901378Z","shell.execute_reply":"2024-02-06T15:01:57.910388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_distribution(dataframe, target_column):\n    # Calculate value counts\n    value_counts = dataframe[target_column].value_counts()\n\n    # Create a figure with two subplots\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n\n    # Bar plot on the first subplot\n    sns.barplot(x=value_counts.index, y=value_counts.values, palette=\"viridis\", ax=ax1)\n    ax1.set_xlabel(target_column, fontsize=12)\n    ax1.set_ylabel('Count', fontsize=12)\n    ax1.set_xticklabels(ax1.get_xticklabels(), rotation=45, ha='right', fontsize=10)\n\n    # Add data labels above each bar\n    for index, value in enumerate(value_counts):\n        ax1.text(index, value, str(value), ha='center', va='bottom', fontsize=10)\n\n    # Pie plot on the second subplot\n    ax2.pie(value_counts, labels=value_counts.index, autopct='%1.1f%%', colors=sns.color_palette(\"viridis\", len(value_counts)))\n    ax2.axis('equal')\n\n    # Main title for the figure\n    fig.suptitle(f'Comparison of {target_column} Distribution in ({dataframe.name})', fontsize=18)\n    \n    # Adjust layout and display the figure\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-06T15:02:01.049751Z","iopub.execute_input":"2024-02-06T15:02:01.050155Z","iopub.status.idle":"2024-02-06T15:02:01.060545Z","shell.execute_reply.started":"2024-02-06T15:02:01.050124Z","shell.execute_reply":"2024-02-06T15:02:01.059121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_distribution(train_data, 'NObeyesdad')","metadata":{"execution":{"iopub.status.busy":"2024-02-06T15:02:20.620039Z","iopub.execute_input":"2024-02-06T15:02:20.620461Z","iopub.status.idle":"2024-02-06T15:02:21.166458Z","shell.execute_reply.started":"2024-02-06T15:02:20.620431Z","shell.execute_reply":"2024-02-06T15:02:21.165324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_distribution(original_data, 'NObeyesdad')","metadata":{"execution":{"iopub.status.busy":"2024-02-06T15:02:26.858721Z","iopub.execute_input":"2024-02-06T15:02:26.859114Z","iopub.status.idle":"2024-02-06T15:02:27.395629Z","shell.execute_reply.started":"2024-02-06T15:02:26.859084Z","shell.execute_reply":"2024-02-06T15:02:27.394414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for column in categorical_vars:\n    plot_distribution(train_data, column)","metadata":{"execution":{"iopub.status.busy":"2024-02-06T15:02:36.53994Z","iopub.execute_input":"2024-02-06T15:02:36.540336Z","iopub.status.idle":"2024-02-06T15:02:39.983999Z","shell.execute_reply.started":"2024-02-06T15:02:36.540308Z","shell.execute_reply":"2024-02-06T15:02:39.982861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_histograms_and_density(dataframe, columns):\n    for column in columns:\n        fig, ax = plt.subplots(figsize=(16, 4))\n        fig = sns.histplot(data=train_data, x=column, hue=\"NObeyesdad\", bins=50, kde=True)\n        plt.ylim(0,500)\n        plt.show()\n        \nplot_histograms_and_density(train_data, continuous_vars)","metadata":{"execution":{"iopub.status.busy":"2024-02-06T09:36:41.754414Z","iopub.execute_input":"2024-02-06T09:36:41.754793Z","iopub.status.idle":"2024-02-06T09:36:50.609609Z","shell.execute_reply.started":"2024-02-06T09:36:41.754767Z","shell.execute_reply":"2024-02-06T09:36:50.608672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.concat([train_data, original_data]).drop(['id'], axis=1).drop_duplicates()\ntest = test_data.drop(['id'], axis=1)\ntrain","metadata":{"execution":{"iopub.status.busy":"2024-02-06T15:02:44.336442Z","iopub.execute_input":"2024-02-06T15:02:44.337231Z","iopub.status.idle":"2024-02-06T15:02:44.410158Z","shell.execute_reply.started":"2024-02-06T15:02:44.337194Z","shell.execute_reply":"2024-02-06T15:02:44.409242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.get_dummies(train,\n                       columns=categorical_vars)\ntest = pd.get_dummies(test, \n                      columns=categorical_vars)\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2024-02-06T15:02:48.024813Z","iopub.execute_input":"2024-02-06T15:02:48.025517Z","iopub.status.idle":"2024-02-06T15:02:48.108146Z","shell.execute_reply.started":"2024-02-06T15:02:48.025482Z","shell.execute_reply":"2024-02-06T15:02:48.106982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train.drop(['NObeyesdad'], axis=1)\ny = train['NObeyesdad']\nX","metadata":{"execution":{"iopub.status.busy":"2024-02-06T15:02:51.231225Z","iopub.execute_input":"2024-02-06T15:02:51.232134Z","iopub.status.idle":"2024-02-06T15:02:51.284863Z","shell.execute_reply.started":"2024-02-06T15:02:51.232083Z","shell.execute_reply":"2024-02-06T15:02:51.283591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y","metadata":{"execution":{"iopub.status.busy":"2024-02-06T15:02:58.174773Z","iopub.execute_input":"2024-02-06T15:02:58.175229Z","iopub.status.idle":"2024-02-06T15:02:58.185493Z","shell.execute_reply.started":"2024-02-06T15:02:58.175193Z","shell.execute_reply":"2024-02-06T15:02:58.184334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Print the shapes of the training and testing datasets\nprint(\"X_train shape:\", X_train.shape)\nprint(\"X_test shape:\", X_test.shape)\nprint(\"y_train shape:\", y_train.shape)\nprint(\"y_test shape:\", y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2024-02-06T15:03:02.984815Z","iopub.execute_input":"2024-02-06T15:03:02.985254Z","iopub.status.idle":"2024-02-06T15:03:02.998781Z","shell.execute_reply.started":"2024-02-06T15:03:02.985222Z","shell.execute_reply":"2024-02-06T15:03:02.99787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #The tuning process has been commented out due to its time-consuming nature.\n\n# # Define the objective function for Optuna optimization\n# def objective(trial, X_train, y_train, X_test, y_test):\n#     # Define parameters to be optimized for the LGBMClassifier\n#     param = {\n#         \"objective\": \"multiclass\",\n#         \"metric\": \"multi_logloss\",\n#         \"verbosity\": -1,\n#         \"boosting_type\": \"gbdt\",\n#         \"random_state\": 42,\n#         \"num_class\": 7,\n#         \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.05),\n#         \"n_estimators\": trial.suggest_int(\"n_estimators\", 400, 600),\n#         \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0.005, 0.015),\n#         \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 0.02, 0.06),\n#         \"max_depth\": trial.suggest_int(\"max_depth\", 6, 14),\n#         \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.3, 0.9),\n#         \"subsample\": trial.suggest_float(\"subsample\", 0.8, 1.0),\n#         \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 10, 50),\n#     }\n\n#     # Create an instance of LGBMClassifier with the suggested parameters\n#     lgbm_classifier = LGBMClassifier(**param)\n    \n#     # Fit the classifier on the training data\n#     lgbm_classifier.fit(X_train, y_train)\n\n#     # Evaluate the classifier on the test data\n#     score = lgbm_classifier.score(X_test, y_test)\n\n#     return score\n\n# # Split the data into training and test sets\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # Adjust the test_size as needed\n\n# # Set up the sampler for Optuna optimization\n# sampler = optuna.samplers.TPESampler(seed=42)  # Using Tree-structured Parzen Estimator sampler for optimization\n\n# # Create a study object for Optuna optimization\n# study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n\n# # Run the optimization process\n# study.optimize(lambda trial: objective(trial, X_train, y_train, X_test, y_test), n_trials=100)\n\n# # Get the best parameters after optimization\n# best_params = study.best_params\n\n# print('='*50)\n# print(best_params)","metadata":{"execution":{"iopub.status.busy":"2024-02-06T09:41:20.53462Z","iopub.execute_input":"2024-02-06T09:41:20.534976Z","iopub.status.idle":"2024-02-06T10:01:13.629726Z","shell.execute_reply.started":"2024-02-06T09:41:20.534951Z","shell.execute_reply":"2024-02-06T10:01:13.62841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Best parameters obtained from Optuna optimization process\n\nbest_params = {\n    \"objective\": \"multiclass\",          # Objective function for the model\n    \"metric\": \"multi_logloss\",          # Evaluation metric\n    \"verbosity\": -1,                    # Verbosity level (-1 for silent)\n    \"boosting_type\": \"gbdt\",            # Gradient boosting type\n    \"random_state\": 42,       # Random state for reproducibility\n    \"num_class\": 7,                     # Number of classes in the dataset\n    'learning_rate': 0.030962211546832760,  # Learning rate for gradient boosting\n    'n_estimators': 500,                # Number of boosting iterations\n    'lambda_l1': 0.009667446568254372,  # L1 regularization term\n    'lambda_l2': 0.04018641437301800,   # L2 regularization term\n    'max_depth': 10,                    # Maximum depth of the trees\n    'colsample_bytree': 0.40977129346872643,  # Fraction of features to consider for each tree\n    'subsample': 0.9535797422450176,    # Fraction of samples to consider for each boosting iteration\n    'min_child_samples': 26             # Minimum number of data needed in a leaf\n}","metadata":{"execution":{"iopub.status.busy":"2024-02-06T15:03:14.496196Z","iopub.execute_input":"2024-02-06T15:03:14.496816Z","iopub.status.idle":"2024-02-06T15:03:14.503695Z","shell.execute_reply.started":"2024-02-06T15:03:14.496785Z","shell.execute_reply":"2024-02-06T15:03:14.502512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbm_classifier = LGBMClassifier(**best_params)\n\nlgbm_classifier.fit(X_train, y_train)\n\ny_pred = lgbm_classifier.predict(X_test)\naccuracy_score(y_test, y_pred) ","metadata":{"execution":{"iopub.status.busy":"2024-02-06T15:03:18.126736Z","iopub.execute_input":"2024-02-06T15:03:18.127182Z","iopub.status.idle":"2024-02-06T15:03:37.977532Z","shell.execute_reply.started":"2024-02-06T15:03:18.127148Z","shell.execute_reply":"2024-02-06T15:03:37.976469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = lgbm_classifier.predict(test)\npredictions","metadata":{"execution":{"iopub.status.busy":"2024-02-06T15:03:37.979344Z","iopub.execute_input":"2024-02-06T15:03:37.979676Z","iopub.status.idle":"2024-02-06T15:03:43.629089Z","shell.execute_reply.started":"2024-02-06T15:03:37.979647Z","shell.execute_reply":"2024-02-06T15:03:43.627791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission['NObeyesdad'] = predictions\nsample_submission","metadata":{"execution":{"iopub.status.busy":"2024-02-06T15:03:43.630614Z","iopub.execute_input":"2024-02-06T15:03:43.630926Z","iopub.status.idle":"2024-02-06T15:03:43.642187Z","shell.execute_reply.started":"2024-02-06T15:03:43.630899Z","shell.execute_reply":"2024-02-06T15:03:43.641281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-02-06T15:03:43.644164Z","iopub.execute_input":"2024-02-06T15:03:43.644547Z","iopub.status.idle":"2024-02-06T15:03:43.683828Z","shell.execute_reply.started":"2024-02-06T15:03:43.644517Z","shell.execute_reply":"2024-02-06T15:03:43.682748Z"},"trusted":true},"execution_count":null,"outputs":[]}]}